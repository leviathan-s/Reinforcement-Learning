{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtlQE866CX+Ru/4nwpewqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leviathan-s/Reinforcement-Learning/blob/main/Cartpole_environment_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import collections\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "JkS79OWDY6oP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.0005\n",
        "gamma = 0.98\n",
        "buffer_limit = 50000\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "43WsJ3bHZCg2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 효율을 상승시키기 위해 replaybuffer 클래스 정의\n",
        "# 데이터 재사용으로 인한 학습 효율 증가 효과\n",
        "# 학습 데이터간의 correlation 억제로 인한 학습 효율 증가 효과\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "        \n",
        "    # 리플레이버퍼에 가장 최근에 들어온 데이터를 저장한다\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "\n",
        "    # 버퍼에서 32개의 미니배치 데이터를 뽑아서 반환한다.\n",
        "    # 트랜지션 데이터의 구성 (s,a,r,s_prime,done_mask)\n",
        "    # 상태 s에서 액션 a를 하니 r의 보상을 받고 s_prime으로 이동하였다. done_mask : 종료상태 여부 표시\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append([done_mask])\n",
        "\n",
        "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), torch.tensor(done_mask_lst)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "knFJnecvZfnE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Qnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Qnet, self).__init__()\n",
        "        self.fc1 = nn.Linear(4,128)\n",
        "        self.fc2 = nn.Linear(128,128)\n",
        "        self.fc3 = nn.Linear(128,2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    # epsilon-greedy 방식으로 실제 실행할 액션을 선택한다\n",
        "    def sample_action(self, obs, epsilon):\n",
        "        out = self.forward(obs)\n",
        "        coin = random.random()\n",
        "        # epsilon의 확률만큼 랜덤액션 선택\n",
        "        if coin < epsilon:\n",
        "            return random.randint(0,1)\n",
        "\n",
        "        # 1 - epsilon의 확률만큼 액션 중 가장 가치가 높은 액션 선택\n",
        "        else:\n",
        "            return out.argmax().item()\n",
        "\n",
        "# 매 episode마다 호출되는 트레이닝 함수\n",
        "def train(q, q_target, memory, optimizer):\n",
        "    # 한 번의 train당 10회의 미니배치를 선택한다\n",
        "    for i in range(10):\n",
        "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
        "\n",
        "        q_out = q(s)\n",
        "        q_a = q_out.gather(1,a)\n",
        "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
        "        target = r + gamma*max_q_prime*done_mask\n",
        "        loss = F.smooth_l1_loss(q_a, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def main():\n",
        "    # 환경 선언\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    # Q Network\n",
        "    q = Qnet()\n",
        "    # Q Target Network\n",
        "    q_target = Qnet()\n",
        "    q_target.load_state_dict(q.state_dict()) # 가중치를 그대로 복사\n",
        "    memory = ReplayBuffer()\n",
        "\n",
        "    print_interval = 20\n",
        "    score = 0.0\n",
        "    optimizer = optim.Adam(q.parameters(), lr = learning_rate)\n",
        "    # optimizer에는 q_target network의 파라미터는 집어넣지 않는다.\n",
        "\n",
        "    # 총 천 회의 에피소드 학습을 진행한다\n",
        "    for n_epi in range(1000):\n",
        "        epsilon = max(0.01, 0.08-0.01*(n_epi/200))\n",
        "        # epsilon 확률값을 8%에서 1%로 선형적으로 감소하게끔 한다\n",
        "        s = env.reset() # 에피소드가 시작할 때마다 상태값 reset\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            a = q.sample_action(torch.from_numpy(s).float(),epsilon)\n",
        "            s_prime, r, done, info = env.step(a)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            memory.put((s,a,r/100.0, s_prime, done_mask)) # r값의 스케일이 너무 커서 100으로 나누어준다\n",
        "            s = s_prime\n",
        "            score += r\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # 리플레이 버퍼가 2000이상으로 충분히 커지면 학습을 시작한다\n",
        "        # 초기 데이터가 많이 재사용되면 학습 효율이 감소하기 때문이다\n",
        "        if memory.size() > 2000:\n",
        "            train(q,q_target,memory, optimizer)\n",
        "\n",
        "\n",
        "        if n_epi%print_interval==0 and n_epi!=0:\n",
        "            q_target.load_state_dict(q.state_dict())\n",
        "            print(\"n_episode : {}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format( \\\n",
        "                n_epi, score/print_interval, memory.size(), epsilon*100))\n",
        "            score=0.0\n",
        "\n",
        "    env.close()\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKVMflUSUpkz",
        "outputId": "378961cd-d8c1-41a9-f907-1fc6de725625"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_episode : 20, score : 10.1, n_buffer : 201, eps : 7.9%\n",
            "n_episode : 40, score : 9.5, n_buffer : 391, eps : 7.8%\n",
            "n_episode : 60, score : 9.7, n_buffer : 585, eps : 7.7%\n",
            "n_episode : 80, score : 9.6, n_buffer : 776, eps : 7.6%\n",
            "n_episode : 100, score : 9.8, n_buffer : 973, eps : 7.5%\n",
            "n_episode : 120, score : 10.1, n_buffer : 1175, eps : 7.4%\n",
            "n_episode : 140, score : 9.8, n_buffer : 1372, eps : 7.3%\n",
            "n_episode : 160, score : 9.6, n_buffer : 1563, eps : 7.2%\n",
            "n_episode : 180, score : 9.4, n_buffer : 1751, eps : 7.1%\n",
            "n_episode : 200, score : 9.6, n_buffer : 1942, eps : 7.0%\n",
            "n_episode : 220, score : 26.2, n_buffer : 2467, eps : 6.9%\n",
            "n_episode : 240, score : 32.5, n_buffer : 3118, eps : 6.8%\n",
            "n_episode : 260, score : 63.9, n_buffer : 4395, eps : 6.7%\n",
            "n_episode : 280, score : 60.9, n_buffer : 5612, eps : 6.6%\n",
            "n_episode : 300, score : 96.8, n_buffer : 7549, eps : 6.5%\n",
            "n_episode : 320, score : 110.7, n_buffer : 9763, eps : 6.4%\n",
            "n_episode : 340, score : 84.7, n_buffer : 11457, eps : 6.3%\n",
            "n_episode : 360, score : 179.2, n_buffer : 15041, eps : 6.2%\n",
            "n_episode : 380, score : 161.1, n_buffer : 18262, eps : 6.1%\n",
            "n_episode : 400, score : 188.2, n_buffer : 22027, eps : 6.0%\n",
            "n_episode : 420, score : 130.2, n_buffer : 24632, eps : 5.9%\n",
            "n_episode : 440, score : 168.1, n_buffer : 27994, eps : 5.8%\n",
            "n_episode : 460, score : 245.8, n_buffer : 32911, eps : 5.7%\n",
            "n_episode : 480, score : 253.7, n_buffer : 37985, eps : 5.6%\n",
            "n_episode : 500, score : 307.5, n_buffer : 44135, eps : 5.5%\n",
            "n_episode : 520, score : 296.6, n_buffer : 50000, eps : 5.4%\n",
            "n_episode : 540, score : 233.1, n_buffer : 50000, eps : 5.3%\n",
            "n_episode : 560, score : 231.2, n_buffer : 50000, eps : 5.2%\n",
            "n_episode : 580, score : 240.8, n_buffer : 50000, eps : 5.1%\n",
            "n_episode : 600, score : 322.4, n_buffer : 50000, eps : 5.0%\n",
            "n_episode : 620, score : 273.9, n_buffer : 50000, eps : 4.9%\n",
            "n_episode : 640, score : 241.3, n_buffer : 50000, eps : 4.8%\n",
            "n_episode : 660, score : 250.3, n_buffer : 50000, eps : 4.7%\n",
            "n_episode : 680, score : 254.2, n_buffer : 50000, eps : 4.6%\n",
            "n_episode : 700, score : 176.7, n_buffer : 50000, eps : 4.5%\n",
            "n_episode : 720, score : 134.7, n_buffer : 50000, eps : 4.4%\n",
            "n_episode : 740, score : 338.4, n_buffer : 50000, eps : 4.3%\n",
            "n_episode : 760, score : 178.1, n_buffer : 50000, eps : 4.2%\n",
            "n_episode : 780, score : 211.9, n_buffer : 50000, eps : 4.1%\n",
            "n_episode : 800, score : 269.1, n_buffer : 50000, eps : 4.0%\n",
            "n_episode : 820, score : 238.5, n_buffer : 50000, eps : 3.9%\n",
            "n_episode : 840, score : 227.7, n_buffer : 50000, eps : 3.8%\n",
            "n_episode : 860, score : 219.8, n_buffer : 50000, eps : 3.7%\n",
            "n_episode : 880, score : 235.9, n_buffer : 50000, eps : 3.6%\n",
            "n_episode : 900, score : 210.5, n_buffer : 50000, eps : 3.5%\n",
            "n_episode : 920, score : 201.8, n_buffer : 50000, eps : 3.4%\n",
            "n_episode : 940, score : 231.1, n_buffer : 50000, eps : 3.3%\n",
            "n_episode : 960, score : 269.8, n_buffer : 50000, eps : 3.2%\n",
            "n_episode : 980, score : 258.2, n_buffer : 50000, eps : 3.1%\n"
          ]
        }
      ]
    }
  ]
}